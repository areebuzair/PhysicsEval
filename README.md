# PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems

This repository contains the codes used to run the experiments of the PhysicsEval paper. In this paper, we explore various inference time techniques to improve the performance of LLMs on Physics and evaluate the performances. 

To run our Problem Solving Pipeline, go to the [BASE SOLUTION](https://github.com/areebuzair/PhysicsEval/tree/main/BASE%20SOLUTION#readme) directory.

To evaluate the solutions generated by our pipeline, enter [EVALUATIONS](https://github.com/areebuzair/PhysicsEval/tree/main/EVALUATIONS#readme) directory.

# PhysicsEval Dataset

To enable large-scale evaluation and training of reasoning-capable language models in physics, we curated a comprehensive dataset of **19,609 annotated problems**, sourced from 20 authoritative physics textbooks and verified educational websites.

The dataset spans 19 different categories, including *Mechanics, Thermodynamics, Electromagnetism, Waves, Optics, Relativity*, and *Quantum Physics*.

It is available at https://huggingface.co/datasets/IUTVanguard/PhysicsEval

### Construction

Each problem is processed through the following pipeline:

- **Data Cleaning**: Raw content is cleaned to remove noise and inconsistencies.
- **LaTeX Annotation**: All equations are converted into LaTeX for structured mathematical representation.
- **Step-Wise Elaboration**: Using Gemini 2.5 Pro in “Think” mode, solutions are decomposed into logically coherent steps to enhance interpretability for LLMs.
- **Metadata Tagging**: Each problem is annotated with topic category, difficulty level, and key physical principles.

**Train-Test Split**: We apply a 90:10 split, resulting in **17,647 training** and **1,962 test samples**, supporting generalization across diverse reasoning tasks.

Each problem is assigned a difficulty score from 1 to 10. The number of steps in the elaborated solution is stored, and in some cases, alternative solution methods are also suggested.

---

### Data Model

Each entry in the dataset includes the following fields:

- `Problem_ID`: Unique identifier for the problem instance  
- `problem`: Original, full problem text from source material  
- `simplified_problem_statement`: Paraphrased version, stripped of complexity  
- `category`: Topical category (e.g., Mechanics, Optics)  
- `soft_labels`: Tags like numerical, conceptual, multi-step, diagram  
- `elaborated_solution_steps`: Step-by-step reasoning to the correct answer  
- `alternative_solutions`: Different valid solution methods  
- `problem_difficulty`: Difficulty rating (1–10)  
- `final_answers_in_brief`: Final answer(s) only, no reasoning  
- `steps`: Number of steps in main solution  
- `source`: The source of the problem 
